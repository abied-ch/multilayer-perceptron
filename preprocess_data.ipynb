{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the data\n",
    "I decided to preprocess the data before splitting it into training/evaluation sets to ensure that there are no scaling discrepancies.\n",
    "\n",
    "## What is this data??\n",
    "\n",
    "By researching where the data might come from, I have been able to find it is the [Breast Cancer Wisconsin (Diagnostic) Dataset](https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data)\n",
    "\n",
    "The dataset has 32 columns:\n",
    "* **id** (ID number)\n",
    "* **diagnosis** (M for malicious, B for benign)\n",
    "* **radius_mean** (mean of distances from center to points on perimeter)\n",
    "* **texture_mean** (standard deviation of gray-scale values)\n",
    "* **perimeter_mean** (mean size of the core tumor)\n",
    "* **area_mean**\n",
    "* **smoothness_mean** (mean of local variation in radius lengths)\n",
    "* **compactness_mean** (mean of perimeter^2 / area - 1.0)\n",
    "* **concavity_mean** (mean of severity of concave portions of the contour)\n",
    "* **concave points_mean** (mean for number of concave portions of the contour)\n",
    "* **symmetry_mean**\n",
    "* **fractal_dimension_mean** (mean for \"coastline approximation\" - 1)\n",
    "* **radius_se** (standard error for the mean of distances from center to points on the perimeter)\n",
    "* **texture_se** (standard error for standard deviation of gray-scale values)\n",
    "* **perimeter_se** \n",
    "* **area_se**\n",
    "* **smoothness_se** (standard error for local variation in radius lengths)\n",
    "* **compactness_se** (standard error for perimeter^2 / area - 1.0)\n",
    "* **concavity_se** (standard error for severity of concave portions of the contour)\n",
    "* **concave points_se** (standard error for number of concave portions of the contour)\n",
    "* **symmetry_se**\n",
    "* **fractal_dimension_se** (standard error for \"coastline approximation\" - 1)\n",
    "* **radius_worst** (\"worst\" or largest mean value for mean of distances from center to points on the perimeter)\n",
    "* **texture_worst** (\"worst\" or largest mean value for standard deviation of gray-scale values)\n",
    "* **perimeter_worst**\n",
    "* **area_worst**\n",
    "* **smoothness_worst** (\"worst\" or largest mean value for local variation in radius lengths)\n",
    "* **compactness_worst** (\"worst\" or largest mean value for perimeter^2 / area - 1.0)\n",
    "* **concavity_worst** (\"worst\" or largest mean value for severity of concave portions of the contour)\n",
    "* **concave points_worst** (\"worst\" or largest mean value for number of concave portions of the contour)\n",
    "* **symmetry_worst**\n",
    "* **fractal_dimension_worst** (\"worst\" or largest mean value for \"coastline approximation\" - 1)\n",
    "\n",
    "I will add these columns to the DataFrame in order to simplify the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
      "0    842302         M        17.99         10.38          122.80     1001.0   \n",
      "1    842517         M        20.57         17.77          132.90     1326.0   \n",
      "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
      "3  84348301         M        11.42         20.38           77.58      386.1   \n",
      "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
      "\n",
      "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
      "0          0.11840           0.27760          0.3001              0.14710   \n",
      "1          0.08474           0.07864          0.0869              0.07017   \n",
      "2          0.10960           0.15990          0.1974              0.12790   \n",
      "3          0.14250           0.28390          0.2414              0.10520   \n",
      "4          0.10030           0.13280          0.1980              0.10430   \n",
      "\n",
      "   ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
      "0  ...         25.38          17.33           184.60      2019.0   \n",
      "1  ...         24.99          23.41           158.80      1956.0   \n",
      "2  ...         23.57          25.53           152.50      1709.0   \n",
      "3  ...         14.91          26.50            98.87       567.7   \n",
      "4  ...         22.54          16.67           152.20      1575.0   \n",
      "\n",
      "   smoothness_worst  compactness_worst  concavity_worst  concave points_worst  \\\n",
      "0            0.1622             0.6656           0.7119                0.2654   \n",
      "1            0.1238             0.1866           0.2416                0.1860   \n",
      "2            0.1444             0.4245           0.4504                0.2430   \n",
      "3            0.2098             0.8663           0.6869                0.2575   \n",
      "4            0.1374             0.2050           0.4000                0.1625   \n",
      "\n",
      "   symmetry_worst  fractal_dimension_worst  \n",
      "0          0.4601                  0.11890  \n",
      "1          0.2750                  0.08902  \n",
      "2          0.3613                  0.08758  \n",
      "3          0.6638                  0.17300  \n",
      "4          0.2364                  0.07678  \n",
      "\n",
      "[5 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data.csv\", header=None) # header=None prevents pandas from interpreting the first data row as the column names.\n",
    "columns = [\n",
    "    \"id\",\n",
    "    \"diagnosis\",\n",
    "    \"radius_mean\",\n",
    "    \"texture_mean\",\n",
    "    \"perimeter_mean\",\n",
    "    \"area_mean\",\n",
    "    \"smoothness_mean\",\n",
    "    \"compactness_mean\",\n",
    "    \"concavity_mean\",\n",
    "    \"concave points_mean\",\n",
    "    \"symmetry_mean\",\n",
    "    \"fractal_dimension_mean\",\n",
    "    \"radius_se\",\n",
    "    \"texture_se\",\n",
    "    \"perimeter_se\",\n",
    "    \"area_se\",\n",
    "    \"smoothness_se\",\n",
    "    \"compactness_se\",\n",
    "    \"concavity_se\",\n",
    "    \"concave points_se\",\n",
    "    \"symmetry_se\",\n",
    "    \"fractal_dimension_se\",\n",
    "    \"radius_worst\",\n",
    "    \"texture_worst\",\n",
    "    \"perimeter_worst\",\n",
    "    \"area_worst\",\n",
    "    \"smoothness_worst\",\n",
    "    \"compactness_worst\",\n",
    "    \"concavity_worst\",\n",
    "    \"concave points_worst\",\n",
    "    \"symmetry_worst\",\n",
    "    \"fractal_dimension_worst\",\n",
    "    ]\n",
    "\n",
    "df.columns = columns\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert categorical values into numerical values\n",
    "Ensure all values can be easily evaluated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical values into numerical.\n",
    "df[\"diagnosis\"] = df[\"diagnosis\"].map({\"M\": 1, \"B\": 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill missing values\n",
    "This data happens to be preprocessed since it comes from a website which provides pre-made Machine Learning datasets.\n",
    "I will still add this step, since it is crucial to ensure missing values have no impact on the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with the mean for their column, ensuring they have no impact on the data.\n",
    "df = df.fillna(df.mean()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "Algorithms which use gradient descent for optimization (like linear regression, logistic regression and in this case, neural networks), converge significantly faster when features are on a similar scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      1\n",
      "1      1\n",
      "2      1\n",
      "3      1\n",
      "4      1\n",
      "      ..\n",
      "564    1\n",
      "565    1\n",
      "566    1\n",
      "567    1\n",
      "568    0\n",
      "Name: diagnosis, Length: 569, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for column in df.columns:\n",
    "    if column != \"diagnosis\":\n",
    "        df[column] = (df[column] - df[column] / df[column].std())\n",
    "\n",
    "print(df[\"diagnosis\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
